{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Capable of performing linear or nonlinear classification, regression, and even outlier detection.\n",
    "\n",
    "SVMs are particularly well suited for classification of complex but __small- or medium-sized__ datasets.\n",
    "\n",
    "## 1. Linear SVM Classification\n",
    "- large margin classification\n",
    "- support vectors\n",
    "- __SVMs are sensitive to the feature scales__\n",
    "\n",
    "### 1.1 Soft Margin Classification\n",
    "In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparameter: a smaller C value\n",
    "leads to a wider street but more margin violations.\n",
    "\n",
    "- If your SVM model is overfitting, you can try regularizing it by reducing C.\n",
    "- Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear SVM\n",
    "# model (using the LinearSVC class with C = 0.1 and the hinge loss function, described shortly) to detect\n",
    "# Iris-Virginica flowers\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "))\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. This is automatic if you scale the data using the StandardScaler.\n",
    "- make sure you set the loss hyperparameter to \"hinge\", as it is not the default value.\n",
    "- for better performance you should set the dual hyperparameter to False, unless there are more features than training instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nonlinear SVM Classification\n",
    "\n",
    "有时数据不是线性可分的，这时一个办法是：增加更多的特征，比如多项式特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make polynomial features\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_svm_clf = Pipeline((\n",
    "(\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "))\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Polynomial Kernel\n",
    "前面通过添加多项式特征的方法，如果多项式度数过大，将会创造太多特征，使模型训练很慢。\n",
    "\n",
    "- 通过kernel trick: It makes it possible to get the same result as if you added many polynomial features, even with very high-degree polynomials, without actually having to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "))\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if your model is overfitting, you might want to reduce the polynomial degree.\n",
    "- The hyperparameter coef0 controls how much the model is influenced by high-degree polynomials versus low-degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding Similarity Features\n",
    "另一种处理非线性可分数据的方法：to add features computed using a _similarity function_\n",
    "\n",
    "- _similarity function_: measures how much each instance resembles a particular landmark\n",
    "-  $\\phi\\gamma(x,l) = exp(-\\gamma\\lVert x-l\\rVert^2)$ \n",
    "- a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark).\n",
    "- The downside不足 is that a training set with m instances and n features gets transformed into a training set with m instances and m features (assuming you drop the original features). If your training set is very large, you end up with an equally large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gaussian RBF Kernel\n",
    "Kernel trick: it makes it possible to obtain a similar result as if you had added many similarity features, without actually having to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=5, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "))\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasing gamma makes the bell-shape curve narrower, and as a result each instance’s range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances.\n",
    "- Conversely, a small gamma value makes the bell-shaped curve wider, so instances have a larger range of influence, and the decision boundary ends up smoother.\n",
    "- γ acts like a regularization hyperparameter: if your model is __overfitting__, you should reduce it, and if it is underfitting, you should increase it (similar to the C hyperparameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Computational Complexity\n",
    "- The LinearSVC class is based on the liblinear library, which implements an optimized algorithm for linear SVMs. \n",
    "It does not support the kernel trick, but it scales almost linearly with the number of training\n",
    "instances and the number of features: its training time complexity is roughly O(m × n).\n",
    "\n",
    "- The algorithm takes longer if you require a very high precision. This is controlled by the tolerance hyperparameter ϵ (called _tol_ in Scikit-Learn). In most classification tasks, the default tolerance is fine.\n",
    "\n",
    "- The SVC class is based on the libsvm library, which implements an algorithm that supports the kernel trick. Unfortunately, this means that it gets dreadfully slow when the number of training instances gets large (e.g., hundreds of thousands of instances).__This algorithm is perfect for complex but small or medium training sets. However, it scales well with the number of features, especially with sparse features (i.e., when each instance has few nonzero features).__\n",
    "\n",
    "| Class | Time complexity | Out-of-core support | Scaling required | Kenerl trick |\n",
    "|------|------|------|------|------|\n",
    "| LinearSVC | O(m×n) | No | Yes | No |\n",
    "| SGDClassifier | O(m×n) | Yes | Yes | No |\n",
    "| SVC | O(m² × n) to O(m³ × n) | No | Yes | Yes|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SVM Regression\n",
    "SVM not only does it support linear and nonlinear classification, but it also supports linear and nonlinear regression.\n",
    "\n",
    "The trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (i.e., instances off the street)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='auto',\n",
       "  kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVMs can also be used for outlier detection; see Scikit-Learn’s documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Under the Hood\n",
    "the bias term will be called b and the feature weights vector will be called w. No bias feature will be added to the input feature vectors.\n",
    "### 4.1 Decision Function and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
